{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d16372-4ccd-47d9-92df-65a377a6dd8e",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "#### Wordpiece tokenizer\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eff03599-0f30-4142-a6e9-8706dd4d6144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a47bd30-f66f-4d87-8df4-a2cec1f67be9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BertTokenizerFast'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1b09b8f-57c6-4813-a0ad-a3aaa12be29a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dummy text\n",
    "text = \"WordPiece tokenizer is powerful for handling subword units.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d341b653-2e20-4633-90b1-c6da6eb77751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: WordPiece tokenizer is powerful for handling subword units.\n",
      "Tokenized Output: ['Word', '##P', '##ie', '##ce', 'token', '##izer', 'is', 'powerful', 'for', 'handling', 'sub', '##word', 'units', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Print the original text and tokenized output\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Tokenized Output:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27308b4-0518-40bc-93e2-3dc3efb927d5",
   "metadata": {},
   "source": [
    "- In `WordPiece` tokenization, words are broken down into subword units, and each subword is prefixed with '##' to indicate that it is part of a larger word. \n",
    "\n",
    "- the `WordPiece` tokenizer decomposes the original text into subword tokens, allowing the model to handle a more diverse range of linguistic units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b179b4d-43f1-409d-aaeb-92cf2abd3729",
   "metadata": {},
   "source": [
    "#### But how does this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "234ab702-eb31-49ed-aa85-647b1e3f4d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "# corpus = [\n",
    "#     \"AI is reshaping industries and daily life.\",\n",
    "#     \"Machine learning analyzes data for predictions.\",\n",
    "#     \"Deep learning mimics the human brain's structure.\",\n",
    "#     \"NLP enables computers to understand human language.\",\n",
    "#     \"Ethical considerations are crucial in AI development.\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e975179-1aa4-4cfa-865f-23411b392464",
   "metadata": {},
   "source": [
    " we compute the frequencies of each word in the corpus as we do the pre-tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2ac0081-555f-4933-85cc-4b0de5a160cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'This': 3,\n",
       "             'is': 2,\n",
       "             'the': 1,\n",
       "             'Hugging': 1,\n",
       "             'Face': 1,\n",
       "             'Course': 1,\n",
       "             '.': 4,\n",
       "             'chapter': 1,\n",
       "             'about': 1,\n",
       "             'tokenization': 1,\n",
       "             'section': 1,\n",
       "             'shows': 1,\n",
       "             'several': 1,\n",
       "             'tokenizer': 1,\n",
       "             'algorithms': 1,\n",
       "             'Hopefully': 1,\n",
       "             ',': 1,\n",
       "             'you': 1,\n",
       "             'will': 1,\n",
       "             'be': 1,\n",
       "             'able': 1,\n",
       "             'to': 1,\n",
       "             'understand': 1,\n",
       "             'how': 1,\n",
       "             'they': 1,\n",
       "             'are': 1,\n",
       "             'trained': 1,\n",
       "             'and': 1,\n",
       "             'generate': 1,\n",
       "             'tokens': 1})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d6902-51a4-4d1d-8ef5-bbbd723703e6",
   "metadata": {},
   "source": [
    "the alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by ##:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aba1a5b4-42d9-4288-a748-c589a6a37fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', '##h', '##i', '##s', 'i', 't', '##e', 'H', '##u', '##g', '##n']\n"
     ]
    }
   ],
   "source": [
    "# First couple of words - illustration\n",
    "alphabet = []\n",
    "c =0 \n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "        \n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "            \n",
    "    if c > 2:        \n",
    "        break\n",
    "    else:\n",
    "        c+=1\n",
    "        \n",
    "print(alphabet)\n",
    "#alphabet.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04c3c0e0-aea5-4c7a-8411-8927f8b87946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for all the words\n",
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "44edcf26-534d-43d0-852b-868ef36a89b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcaa175-4105-4151-83b1-c77bf8b25544",
   "metadata": {},
   "source": [
    "We also add the special tokens used by the model at the beginning of that vocabulary. \n",
    "\n",
    "In the case of BERT, it’s the list [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1eda899f-20f6-4105-8b5a-32fbb4d9a802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "beecfeef-b48b-463e-8a42-05b04ed3ffbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee4f33-42ab-4ff9-81ce-815bad804e1b",
   "metadata": {},
   "source": [
    "Next we need to split each word, with all the letters that are not the first prefixed by ##:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b30ed9ef-8efc-4b76-8705-79e4e469ac81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0fbb3bb5-49f0-46b4-9e87-31d87f7e0623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['T', '##h', '##i', '##s'],\n",
       " 'is': ['i', '##s'],\n",
       " 'the': ['t', '##h', '##e'],\n",
       " 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'],\n",
       " 'Face': ['F', '##a', '##c', '##e'],\n",
       " 'Course': ['C', '##o', '##u', '##r', '##s', '##e'],\n",
       " '.': ['.'],\n",
       " 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'],\n",
       " 'about': ['a', '##b', '##o', '##u', '##t'],\n",
       " 'tokenization': ['t',\n",
       "  '##o',\n",
       "  '##k',\n",
       "  '##e',\n",
       "  '##n',\n",
       "  '##i',\n",
       "  '##z',\n",
       "  '##a',\n",
       "  '##t',\n",
       "  '##i',\n",
       "  '##o',\n",
       "  '##n'],\n",
       " 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'],\n",
       " 'shows': ['s', '##h', '##o', '##w', '##s'],\n",
       " 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'],\n",
       " 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'],\n",
       " 'algorithms': ['a',\n",
       "  '##l',\n",
       "  '##g',\n",
       "  '##o',\n",
       "  '##r',\n",
       "  '##i',\n",
       "  '##t',\n",
       "  '##h',\n",
       "  '##m',\n",
       "  '##s'],\n",
       " 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'],\n",
       " ',': [','],\n",
       " 'you': ['y', '##o', '##u'],\n",
       " 'will': ['w', '##i', '##l', '##l'],\n",
       " 'be': ['b', '##e'],\n",
       " 'able': ['a', '##b', '##l', '##e'],\n",
       " 'to': ['t', '##o'],\n",
       " 'understand': ['u',\n",
       "  '##n',\n",
       "  '##d',\n",
       "  '##e',\n",
       "  '##r',\n",
       "  '##s',\n",
       "  '##t',\n",
       "  '##a',\n",
       "  '##n',\n",
       "  '##d'],\n",
       " 'how': ['h', '##o', '##w'],\n",
       " 'they': ['t', '##h', '##e', '##y'],\n",
       " 'are': ['a', '##r', '##e'],\n",
       " 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'],\n",
       " 'and': ['a', '##n', '##d'],\n",
       " 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'],\n",
       " 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94586ed-aebc-4487-a0f8-c943dd53d6fc",
   "metadata": {},
   "source": [
    "Now that we are ready for training, let’s write a function that computes the score of each pair. \n",
    "\n",
    "We’ll need to use this at each step of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "27218923-8a99-4d2e-8846-2391403e4b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    \n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs   = defaultdict(int)\n",
    "    \n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6efc3-bdcd-4787-a9a9-ffa1a2a0829a",
   "metadata": {
    "tags": []
   },
   "source": [
    "let’s have a look at a part of this dictionary after the initial splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "280b4ade-c4e4-437f-93b5-94e0a1ed4073",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', '##h'): 0.125\n",
      "('##h', '##i'): 0.03409090909090909\n",
      "('##i', '##s'): 0.02727272727272727\n",
      "('i', '##s'): 0.1\n",
      "('t', '##h'): 0.03571428571428571\n",
      "('##h', '##e'): 0.011904761904761904\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145cc14-74ad-4b20-b4fd-5f5a58495225",
   "metadata": {},
   "source": [
    "Now, finding the pair with the best score only takes a quick loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "12f44fca-3f98-4584-af4d-0316d7ba5e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "48f99cbc-f50f-49ac-a712-2d53d9d7d293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '##b') 0.2\n"
     ]
    }
   ],
   "source": [
    "print(best_pair, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e433c-f0e1-4d56-b1f1-ee205ac63c32",
   "metadata": {},
   "source": [
    "So the first merge to learn is ('a', '##b') -> 'ab', and we add 'ab' to the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c4e3be3-d93c-4e04-8516-e93370154dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab.append(\"ab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94951542-a8ce-465d-8223-8d038371382f",
   "metadata": {},
   "source": [
    "To continue, we need to apply that merge in our splits dictionary. Let’s write another function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7a81e05d-4c6a-4475-a671-20b58b54f365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855a3f7-fc3d-4728-be0a-48fa0efb7a53",
   "metadata": {
    "tags": []
   },
   "source": [
    "And we can have a look at the result of the first merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55f946b8-2c62-4303-b6ae-51e0e4b250e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', '##o', '##u', '##t']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"a\", \"##b\", splits)\n",
    "splits[\"about\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c2610d-a669-468b-a567-fba2eb823f6b",
   "metadata": {},
   "source": [
    "Now we have everything we need to loop until we have learned all the merges we want. Let’s aim for a vocab size of 70:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1723d818-3439-4134-bc24-22b2d5301ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "230745fc-b6c3-44d8-bb18-47df71c2967f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b09325-256a-4814-b8ba-e65f24c49eee",
   "metadata": {
    "tags": []
   },
   "source": [
    "compared to BPE, this tokenizer learns parts of words as tokens a bit faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a84e73-a801-4668-9daf-ab6efbca2b02",
   "metadata": {},
   "source": [
    "To tokenize a new text, we pre-tokenize it, split it, then apply the tokenization algorithm on each word. That is, we look for the biggest subword starting at the beginning of the first word and split it, then we repeat the process on the second part, and so on for the rest of that word and the following words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "32845b52-60a8-4379-a920-3b4e2c9a0f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79187867-1e40-4b6d-baba-61de969a2c13",
   "metadata": {},
   "source": [
    "Let’s test it on one word that’s in the vocabulary, and another that isn’t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a0b9f0df-ff34-491b-a07f-b10cf43864cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hugg', '##i', '##n', '##g']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hugging\"))\n",
    "print(encode_word(\"HOgging\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a7851c-c362-4751-8bb9-cd003466a378",
   "metadata": {},
   "source": [
    "Now, let’s write a function that tokenizes a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2302bb44-bcf3-4122-8e8e-486c3b320624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6113007b-15a7-4625-9f46-c6a8243be7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hugg',\n",
       " '##i',\n",
       " '##n',\n",
       " '##g',\n",
       " '[UNK]',\n",
       " 'h',\n",
       " '##a',\n",
       " '##s',\n",
       " 'g',\n",
       " '##o',\n",
       " '##o',\n",
       " '##d',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Hugging face has good models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58622f-8b2e-448f-9582-7b3bbe11d8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c05ba-bee6-48f4-948b-a97b51313c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
