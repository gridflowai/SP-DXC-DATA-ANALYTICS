{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "### Dropout is a regularization technique \n",
    "- commonly used in deep learning models, including those built with Keras, to prevent overfitting. \n",
    "- Overfitting occurs when a neural network learns to perform well on the training data but struggles to generalize to unseen data. \n",
    "- Dropout helps mitigate this problem by randomly deactivating (dropping out) a fraction of neurons during each training iteration. \n",
    "- This prevents any single neuron from becoming too reliant on specific inputs and forces the network to learn more robust features.\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, you can apply dropout to layers using the Dropout layer or by setting the dropout parameter in certain layers like Dense and LSTM. \n",
    "\n",
    "Here's how you can use dropout in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784).astype('float32') / 255\n",
    "X_test  = X_test.reshape(X_test.shape[0], 784).astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(y_train)  # One-hot encode labels\n",
    "y_test  = to_categorical(y_test)    # One-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a simple feedforward neural network with dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=784, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout with a rate of 0.5\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout with a rate of 0.5\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss     ='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics  =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 9s 16ms/step - loss: 0.3872 - accuracy: 0.8805 - val_loss: 0.1334 - val_accuracy: 0.9580\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.1771 - accuracy: 0.9475 - val_loss: 0.0997 - val_accuracy: 0.9691\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1387 - accuracy: 0.9595 - val_loss: 0.0824 - val_accuracy: 0.9744\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.1170 - accuracy: 0.9647 - val_loss: 0.0778 - val_accuracy: 0.9763\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1002 - accuracy: 0.9693 - val_loss: 0.0682 - val_accuracy: 0.9795\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.0934 - accuracy: 0.9714 - val_loss: 0.0659 - val_accuracy: 0.9796\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0833 - accuracy: 0.9742 - val_loss: 0.0661 - val_accuracy: 0.9806\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0755 - accuracy: 0.9761 - val_loss: 0.0619 - val_accuracy: 0.9817\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0750 - accuracy: 0.9769 - val_loss: 0.0588 - val_accuracy: 0.9803\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0694 - accuracy: 0.9787 - val_loss: 0.0614 - val_accuracy: 0.9817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2028aa4e1d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=10, \n",
    "          batch_size=128, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0614 - accuracy: 0.9817\n",
      "Test loss: 0.0614, Test accuracy: 0.9817\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips For Using Dropout\n",
    "\n",
    "- Generally, use a small dropout value of 20%-50% of neurons with 20% providing a good starting point. A probability too low has minimal effect and a value too high results in under-learning by the network.\n",
    "\n",
    "- Use a __larger network__. You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\n",
    "\n",
    "- Use dropout on __incoming__ (visible) as well as __hidden__ units. Application of dropout at each layer of the network has shown good results.\n",
    "\n",
    "- Use a __large learning rate__ with __decay__ and a __large momentum__. \n",
    "\n",
    "- Increase your learning rate by a factor of 10 to 100 and use a high momentum value of 0.9 or 0.99.\n",
    "\n",
    "- Constrain the size of network weights. A large learning rate can result in very large network weights. Imposing a constraint on the size of network weights such as max-norm regularization with a size of 4 or 5 has been shown to improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
